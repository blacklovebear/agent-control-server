# canal -> source -> memoryChannel -> kafkaSink

agent.sources = $sourceNames
agent.channels = memoryChannel
agent.sinks = kafkaSink


#foreach($source in $sources)

#if($multiTopicJob)
agent.sources.$source.sourceName#[[.]]#type = com.citic.source.canal.CanalSource
#else
agent.sources.$source.sourceName#[[.]]#type = com.citic.source.canal.TransCanalSource
#end

# get local ip for data monitor
agent.sources.$source.sourceName#[[.]]#ipInterface = eth0

# zookeeper servers
agent.sources.$source.sourceName#[[.]]#zkServers = $sourceZkServers

# canal destination
agent.sources.$source.sourceName#[[.]]#destination = $source.sourceDestination

# agent.sources.$source.sourceName#[[.]]#username = user
# agent.sources.$source.sourceName#[[.]]#password = password

# binlog batch size, default is 1024
agent.sources.$source.sourceName#[[.]]#batchSize = 1024


# db.table_name:topic_name:schema_name;db.table_name:topic_name:schema_name
agent.sources.$source.sourceName#[[.]]#tableToTopicMap = $source.tableToTopicMap

# if topic_name starts with "mix_" means mix topic, table field only have "data"
# id|id1,name|name1;uid|uid2,name|name2
agent.sources.$source.sourceName#[[.]]#tableFieldsFilter = $source.tableFieldsFilter
# test\\..*;test1.test2;test1.test3:id,name
agent.sources.$source.sourceName#[[.]]#removeFilter = $source.removeFilter

agent.sources.$source.sourceName#[[.]]#channels = memoryChannel

agent.sources.$source.sourceName#[[.]]#useAvro = $useAvro
agent.sources.$source.sourceName#[[.]]#shutdownFlowCounter = false
#end


agent.sinks.kafkaSink.channel = memoryChannel
agent.sinks.kafkaSink.type = com.citic.sink.canal.KafkaSink
agent.sinks.kafkaSink.kafka.topic = canal_test
agent.sinks.kafkaSink.kafka.bootstrap.servers = $sinkServers

#if($useAvro)
agent.sinks.kafkaSink.kafka.registryUrl = $registryUrl
#end
agent.sinks.kafkaSink.kafka.sendErrorFile = logs/send-error.log

agent.sinks.kafkaSink.allowTopicOverride = true
agent.sinks.kafkaSink.topicHeader = topic
agent.sinks.kafkaSink.flumeBatchSize = 1024
agent.sinks.kafkaSink.useAvroEventFormat = $useAvro
agent.sinks.kafkaSink.countMonitorInterval = 5

agent.sinks.kafkaSink.kafka.producer.compression.type = snappy

#if($kafkaHighThroughput)
# 高吞吐
agent.sinks.kafkaSink.kafka.producer.linger.ms = 1
agent.sinks.kafkaSink.kafka.producer.acks = 1
agent.sinks.kafkaSink.kafka.producer.retries = 3
agent.sinks.kafkaSink.kafka.producer.max.in.flight.requests.per.connection = 5
#else
# 保证消息准确性以及保证顺序
agent.sinks.kafkaSink.kafka.producer.block.on.buffer.full = true
agent.sinks.kafkaSink.kafka.producer.acks = all
# 最大值,无限次重试
# max 2147483647
agent.sinks.kafkaSink.kafka.producer.retries = 10
agent.sinks.kafkaSink.kafka.producer.max.in.flight.requests.per.connection = 1
agent.sinks.kafkaSink.kafka.producer.enable.idempotence = true
#end

# 记得打开 conf/flume-env.sh 中的 JAVA_OPTS
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 1000000
agent.channels.memoryChannel.transactionCapacity = 100000
agent.channels.memoryChannel.byteCapacityBufferPercentage = 20