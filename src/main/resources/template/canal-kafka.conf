# canal -> source -> memoryChannel -> kafkaSink

agent.sources = canalSource
agent.channels = memoryChannel
agent.sinks = kafkaSink

agent.sources.canalSource.type = com.citic.source.canal.CanalSource

# zookeeper servers
agent.sources.canalSource.zkServers = $sourceZkServers

# canal destination
agent.sources.canalSource.destination = $sourceDestination

# agent.sources.canalSource.username = user
# agent.sources.canalSource.password = password

# binlog batch size, default is 1024
agent.sources.canalSource.batchSize = 1024

# 是否需要修改前的数据
agent.sources.canalSource.oldDataRequired = true

#schema.table_name:topic_name;schema.table_name:topic_name
agent.sources.canalSource.tableToTopicMap = $tableToTopicMap

#schema.table_name:field_name,field_name;schema.table_name:field_name,field_name
agent.sources.canalSource.tableFieldsFilter = $tableFieldsFilter


agent.sources.canalSource.channels = memoryChannel

agent.sinks.kafkaSink.channel = memoryChannel
agent.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.kafkaSink.kafka.topic = canal_test
agent.sinks.kafkaSink.kafka.bootstrap.servers = $sinkServers
agent.sinks.kafkaSink.allowTopicOverride = true
agent.sinks.kafkaSink.topicHeader = topic
agent.sinks.kafkaSink.flumeBatchSize = 20
agent.sinks.kafkaSink.kafka.producer.acks = 1
agent.sinks.kafkaSink.kafka.producer.linger.ms = 1
agent.sinks.kafkaSink.kafka.producer.compression.type = snappy


agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 100
