# canal -> source -> memoryChannel -> kafkaSink

agent.sources = $sourceNames
agent.channels = memoryChannel
agent.sinks = kafkaSink


#foreach($source in $sources)

agent.sources.$source.sourceName#[[.]]#type = com.citic.source.canal.CanalSource

# get local ip for data monitor
agent.sources.$source.sourceName#[[.]]#ipInterface = eth0

# zookeeper servers
agent.sources.$source.sourceName#[[.]]#zkServers = $sourceZkServers

# canal destination
agent.sources.$source.sourceName#[[.]]#destination = $source.sourceDestination

# agent.sources.$source.sourceName#[[.]]#username = user
# agent.sources.$source.sourceName#[[.]]#password = password

# binlog batch size, default is 1024
agent.sources.$source.sourceName#[[.]]#batchSize = 1024

#schema.table_name:topic_name;schema.table_name:topic_name
agent.sources.$source.sourceName#[[.]]#tableToTopicMap = $source.tableToTopicMap

#schema.table_name:field_name,field_name;schema.table_name:field_name,field_name
agent.sources.$source.sourceName#[[.]]#tableFieldsFilter = $source.tableFieldsFilter
agent.sources.$source.sourceName#[[.]]#channels = memoryChannel

#end



agent.sinks.kafkaSink.channel = memoryChannel
agent.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.kafkaSink.kafka.topic = canal_test
agent.sinks.kafkaSink.kafka.bootstrap.servers = $sinkServers
agent.sinks.kafkaSink.kafka.registryUrl = $registryUrl
agent.sinks.kafkaSink.allowTopicOverride = true
agent.sinks.kafkaSink.topicHeader = topic
agent.sinks.kafkaSink.flumeBatchSize = 20
agent.sinks.kafkaSink.kafka.producer.acks = 1
agent.sinks.kafkaSink.kafka.producer.linger.ms = 1
agent.sinks.kafkaSink.kafka.producer.compression.type = snappy


agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 100
